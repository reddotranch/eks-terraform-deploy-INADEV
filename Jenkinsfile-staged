def COLOR_MAP = [
    'SUCCESS': 'good', 
    'FAILURE': 'danger',
]

pipeline {
    agent { node { label 'TERRAFORM' } } 
    parameters {
        choice(name: 'Deployment_Type', choices:['apply','destroy'], description:'The deployment type')
        choice(name: 'Manual_Approval', choices: ['Approve','Reject'], description: 'Approve or Reject the deployment')
    }
    environment {
        EMAIL_TO = 'betechincorporated@gmail.com'
        AWS_REGION = 'us-west-2'
    }
    stages {
        stage('1. Terraform Init') {
            steps {
                echo 'Terraform init phase'
                sh '''
                echo 'Initializing main terraform configuration...'
                terraform init || {
                    echo "Initial terraform init failed, attempting state recovery..."
                    
                    # Clean local state
                    rm -rf .terraform/
                    rm -f .terraform.lock.hcl
                    
                    # Check if this is a checksum mismatch issue
                    if terraform init 2>&1 | grep -q "checksum calculated for the state stored in S3 does not match"; then
                        echo "Detected S3/DynamoDB state checksum mismatch, attempting recovery..."
                        
                        # Try to fix the DynamoDB digest entry - try multiple possible key formats
                        echo "Removing potentially corrupted digest entries from DynamoDB..."
                        
                        # Try different possible digest key formats
                        aws dynamodb delete-item \
                            --table-name terraform-state-lock-table \
                            --key '{"LockID":{"S":"west-betech-tfstate/infra/terraformstatefile-md5"}}' \
                            --region us-west-2 2>/dev/null || echo "Digest entry format 1 not found"
                        
                        aws dynamodb delete-item \
                            --table-name terraform-state-lock-table \
                            --key '{"LockID":{"S":"west-betech-tfstate/infra/terraformstatefile"}}' \
                            --region us-west-2 2>/dev/null || echo "Digest entry format 2 not found"
                        
                        # List all items in the table to see what's there
                        echo "Checking DynamoDB table contents..."
                        aws dynamodb scan \
                            --table-name terraform-state-lock-table \
                            --region us-west-2 \
                            --query 'Items[*].LockID.S' \
                            --output text 2>/dev/null || echo "Could not scan table"
                        
                        # Clear any lock entries that might be stuck
                        echo "Clearing any stuck lock entries..."
                        aws dynamodb delete-item \
                            --table-name terraform-state-lock-table \
                            --key '{"LockID":{"S":"west-betech-tfstate/infra/terraformstatefile"}}' \
                            --region us-west-2 2>/dev/null || echo "No lock entry to clear"
                        
                        # Force clear the entire DynamoDB table if needed (last resort)
                        echo "Performing full table cleanup..."
                        ITEMS=$(aws dynamodb scan --table-name terraform-state-lock-table --region us-west-2 --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")
                        if [ "$ITEMS" != "" ] && [ "$ITEMS" != "None" ]; then
                            echo "Found DynamoDB entries, clearing them: $ITEMS"
                            for item in $ITEMS; do
                                if [ "$item" != "" ] && [ "$item" != "None" ]; then
                                    echo "Deleting DynamoDB item: $item"
                                    aws dynamodb delete-item \
                                        --table-name terraform-state-lock-table \
                                        --key "{\"LockID\":{\"S\":\"$item\"}}" \
                                        --region us-west-2 2>/dev/null || echo "Could not delete $item"
                                fi
                            done
                            echo "Completed DynamoDB cleanup"
                        else
                            echo "No DynamoDB entries found to clean up"
                        fi
                        
                        # Wait for DynamoDB to process
                        sleep 15
                        
                        # Verify DynamoDB is actually clean
                        echo "Verifying DynamoDB cleanup..."
                        REMAINING_ITEMS=$(aws dynamodb scan --table-name terraform-state-lock-table --region us-west-2 --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")
                        if [ "$REMAINING_ITEMS" != "" ] && [ "$REMAINING_ITEMS" != "None" ]; then
                            echo "Warning: DynamoDB still has items after cleanup: $REMAINING_ITEMS"
                            echo "Attempting secondary cleanup..."
                            for item in $REMAINING_ITEMS; do
                                if [ "$item" != "" ] && [ "$item" != "None" ]; then
                                    echo "Secondary cleanup of DynamoDB item: $item"
                                    aws dynamodb delete-item \
                                        --table-name terraform-state-lock-table \
                                        --key "{\"LockID\":{\"S\":\"$item\"}}" \
                                        --region us-west-2 2>/dev/null || echo "Could not delete $item in secondary cleanup"
                                fi
                            done
                            sleep 10
                        else
                            echo "DynamoDB cleanup verified - table is clean"
                        fi
                        
                        # Try init again
                        echo "Retrying terraform init after digest cleanup..."
                        if ! terraform init; then
                            echo "Terraform init still failing, attempting state backup and recovery..."
                            
                            # Backup current state from S3
                            echo "Backing up current state file..."
                            aws s3 cp s3://west-betech-tfstate/infra/terraformstatefile ./terraform.tfstate.backup-$(date +%Y%m%d_%H%M%S) 2>/dev/null || echo "No state file to backup"
                            
                            # Remove corrupted state temporarily
                            echo "Temporarily removing state file for fresh init..."
                            aws s3 rm s3://west-betech-tfstate/infra/terraformstatefile 2>/dev/null || echo "No state file to remove"
                            
                            # Try init with empty state
                            echo "Initializing with clean state..."
                            if terraform init; then
                                echo "Successfully initialized with clean state"
                                
                                # If we have infrastructure running, try to import it
                                echo "Checking if infrastructure exists and needs to be imported..."
                                
                                # Check if VPC exists
                                VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
                                if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "" ]; then
                                    echo "Found existing VPC: $VPC_ID"
                                    echo "Infrastructure exists but state is lost. You may need to:"
                                    echo "1. Import existing resources manually, or"
                                    echo "2. Run destroy to clean up orphaned resources, or" 
                                    echo "3. Restore state from backup if available"
                                    echo "Continuing with current clean state..."
                                fi
                            else
                                echo "All recovery attempts failed. Manual intervention required."
                                echo "=== MANUAL RECOVERY STEPS ==="
                                echo "1. Go to AWS Console -> DynamoDB -> Tables -> terraform-state-lock-table"
                                echo "2. Delete ALL items in the table (if any exist)"
                                echo "3. Go to AWS Console -> S3 -> west-betech-tfstate bucket"
                                echo "4. Delete the file: infra/terraformstatefile (if it exists)"
                                echo "5. Re-run this pipeline"
                                echo "================================"
                                echo "If the issue persists, the DynamoDB table or S3 bucket may need to be recreated."
                                exit 1
                            fi
                        fi
                    else
                        echo "Different terraform init error, investigating..."
                        # Check if it's a permissions or other issue
                        terraform init -no-color 2>&1 | tee init_error.log || {
                            echo "Terraform init failed. Error details:"
                            cat init_error.log
                            echo "Checking AWS credentials and permissions..."
                            aws sts get-caller-identity || echo "AWS credentials issue detected"
                            echo "Checking S3 bucket access..."
                            aws s3 ls s3://west-betech-tfstate/ || echo "S3 bucket access issue detected"
                            echo "Checking DynamoDB table access..."
                            aws dynamodb describe-table --table-name terraform-state-lock-table --region us-west-2 || echo "DynamoDB table access issue detected"
                            exit 1
                        }
                    fi
                }
                
                echo 'Terraform initialization completed successfully'
                '''
            }
        }
        
        stage('2. Terraform Plan') {
            steps {
                echo 'Terraform plan phase'
                sh '''
                echo 'Running terraform plan for main configuration...'
                terraform plan -out=tfplan || {
                    echo "Terraform plan failed!"
                    exit 1
                }
                
                echo 'Terraform plan completed successfully'
                echo 'Plan summary:'
                terraform show -no-color tfplan | head -20
                '''
            }
        }

        stage('3. Manual Approval') {
            steps {
                script {
                    def Manual_Approval = params.Manual_Approval
                    echo "Deployment ${Manual_Approval}"

                    if (Manual_Approval == 'Reject') {
                        error "Deployment rejected, stopping pipeline."
                    } 
                }  
            }
        }

        stage('4. Terraform Deploy - Stage 1 (Infrastructure)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 1: VPC and EKS cluster'  
                sh '''
                echo 'Applying terraform plan for VPC and EKS cluster...'
                terraform apply -target=module.vpc -target=module.eks tfplan || {
                    echo "Stage 1 deployment failed!"
                    exit 1
                }
                
                echo 'Stage 1 deployment completed successfully'
                echo 'Checking terraform outputs...'
                terraform output || echo "No outputs available yet"
                '''
                
                echo 'Configuring kubectl'
                sh '''
                # Get cluster name with fallback
                CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "betech-cluster")
                echo "Configuring kubectl for cluster: $CLUSTER_NAME"
                
                aws eks update-kubeconfig --region ${AWS_REGION} --name $CLUSTER_NAME || {
                    echo "Failed to update kubeconfig for $CLUSTER_NAME"
                    exit 1
                }
                
                echo "Kubectl configured successfully"
                kubectl config current-context
                '''
                
                echo 'Waiting for cluster to be ready'
                sh '''
                echo "Waiting for cluster nodes to be ready..."
                kubectl wait --for=condition=Ready nodes --all --timeout=600s || {
                    echo "Nodes not ready yet, but continuing with deployment..."
                    kubectl get nodes || echo "Cannot get nodes yet"
                }
                '''
            }
        }
        
        stage('5. Terraform Deploy - Stage 2 (Kubernetes)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 2: Kubernetes resources and ALB Controller'
                
                script {
                    // Function to get terraform output with fallback
                    def getTerraformOutput = { outputName, fallback ->
                        try {
                            def output = sh(script: "terraform output -raw ${outputName}", returnStdout: true).trim()
                            if (output && output != 'null' && output != '') {
                                return output
                            } else {
                                echo "Warning: Output '${outputName}' is empty, using fallback: ${fallback}"
                                return fallback
                            }
                        } catch (Exception e) {
                            echo "Warning: Could not get terraform output '${outputName}': ${e.getMessage()}"
                            echo "Using fallback: ${fallback}"
                            return fallback
                        }
                    }
                    
                    // Get outputs from stage 1 with fallbacks
                    def cluster_name = getTerraformOutput('cluster_name', 'betech-cluster')
                    def main_region = getTerraformOutput('main_region', 'us-west-2')
                    def vpc_id = getTerraformOutput('vpc_id', '')
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // If VPC ID is still empty, try to get it from the cluster
                    if (!vpc_id || vpc_id == '') {
                        echo "VPC ID not found in terraform outputs, retrieving from cluster..."
                        try {
                            vpc_id = sh(script: "aws eks describe-cluster --name ${cluster_name} --region ${main_region} --query 'cluster.resourcesVpcConfig.vpcId' --output text", returnStdout: true).trim()
                            echo "Retrieved VPC ID from cluster: ${vpc_id}"
                        } catch (Exception e) {
                            echo "Warning: Could not retrieve VPC ID from cluster: ${e.getMessage()}"
                            vpc_id = "vpc-unknown"
                        }
                    }
                    
                    echo "Using values: cluster=${cluster_name}, region=${main_region}, vpc=${vpc_id}"
                    
                    // Ensure kubectl is properly configured and working
                    sh """
                    echo 'Verifying kubectl configuration...'
                    kubectl config current-context
                    kubectl get nodes
                    kubectl cluster-info
                    """
                    
                    // Create terraform.tfvars for stage 2
                    sh """
                    # Ensure stage2-kubernetes directory exists
                    if [ ! -d "stage2-kubernetes" ]; then
                        echo "Error: stage2-kubernetes directory not found!"
                        exit 1
                    fi
                    
                    cd stage2-kubernetes
                    
                    # Create terraform.tfvars with proper validation
                    echo "Creating terraform.tfvars for stage 2..."
                    
                    # Build the tfvars content step by step
                    {
                        echo 'main-region = "${main_region}"'
                        echo 'env_name = "betech"'
                        echo 'cluster_name = "${cluster_name}"'
                        echo 'rolearn = "arn:aws:iam::${account_id}:role/terraform-poweruser"'
                        if [ "${vpc_id}" != "" ] && [ "${vpc_id}" != "vpc-unknown" ]; then
                            echo 'vpc_id = "${vpc_id}"'
                        fi
                    } > terraform.tfvars
                    
                    if [ "${vpc_id}" != "" ] && [ "${vpc_id}" != "vpc-unknown" ]; then
                        echo "Added VPC ID to terraform.tfvars: ${vpc_id}"
                    else
                        echo "Warning: VPC ID not available or invalid, stage2 may need to discover it"
                    fi
                    
                    echo "Contents of terraform.tfvars:"
                    cat terraform.tfvars
                    
                    # Set environment variables for Kubernetes/Helm providers
                    export KUBE_CONFIG_PATH=\$HOME/.kube/config
                    export KUBECONFIG=\$HOME/.kube/config
                    
                    echo 'Setting up Helm for AWS Load Balancer Controller...'
                    # Initialize Helm if needed
                    if ! helm version >/dev/null 2>&1; then
                        echo "Helm not found, installing..."
                        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
                    fi
                    
                    # Add required Helm repositories
                    helm repo list | grep eks || helm repo add eks https://aws.github.io/eks-charts
                    helm repo update
                    
                    echo 'Initializing stage 2 terraform...'
                    terraform init || {
                        echo "Stage 2 terraform init failed, attempting state recovery..."
                        
                        # Clean local state
                        rm -rf .terraform/
                        rm -f .terraform.lock.hcl
                        
                        # Check if this is a checksum mismatch issue for stage 2
                        if terraform init 2>&1 | grep -q "checksum calculated for the state stored in S3 does not match"; then
                            echo "Detected Stage 2 S3/DynamoDB state checksum mismatch, attempting recovery..."
                            
                            # Try to fix the DynamoDB digest entries for stage 2
                            echo "Removing potentially corrupted Stage 2 digest entries from DynamoDB..."
                            
                            # Try different possible digest key formats for stage 2
                            aws dynamodb delete-item \
                                --table-name terraform-state-lock-table \
                                --key '{"LockID":{"S":"west-betech-tfstate/infra/stage2/terraformstatefile-md5"}}' \
                                --region us-west-2 2>/dev/null || echo "Stage 2 digest entry format 1 not found"
                            
                            aws dynamodb delete-item \
                                --table-name terraform-state-lock-table \
                                --key '{"LockID":{"S":"west-betech-tfstate/infra/stage2/terraformstatefile"}}' \
                                --region us-west-2 2>/dev/null || echo "Stage 2 digest entry format 2 not found"
                            
                            # List all items in the table to see what's there for stage 2
                            echo "Checking DynamoDB table contents for Stage 2..."
                            aws dynamodb scan \
                                --table-name terraform-state-lock-table \
                                --region us-west-2 \
                                --query 'Items[*].LockID.S' \
                                --output text 2>/dev/null || echo "Could not scan table"
                            
                            # Clear any lock entries that might be stuck for stage 2
                            echo "Clearing any stuck Stage 2 lock entries..."
                            aws dynamodb delete-item \
                                --table-name terraform-state-lock-table \
                                --key '{"LockID":{"S":"west-betech-tfstate/infra/stage2/terraformstatefile"}}' \
                                --region us-west-2 2>/dev/null || echo "No Stage 2 lock entry to clear"
                            
                            # Force clear any stage 2 related entries from DynamoDB table if needed
                            echo "Performing Stage 2 table cleanup..."
                            ITEMS=\$(aws dynamodb scan --table-name terraform-state-lock-table --region us-west-2 --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")
                            if [ "\$ITEMS" != "" ] && [ "\$ITEMS" != "None" ]; then
                                echo "Found DynamoDB entries, checking for Stage 2 related items: \$ITEMS"
                                for item in \$ITEMS; do
                                    if [ "\$item" != "" ] && [ "\$item" != "None" ] && echo "\$item" | grep -q "stage2"; then
                                        echo "Deleting Stage 2 related DynamoDB item: \$item"
                                        aws dynamodb delete-item \
                                            --table-name terraform-state-lock-table \
                                            --key "{\\"LockID\\":{\\"S\\":\\"\$item\\"}}" \
                                            --region us-west-2 2>/dev/null || echo "Could not delete \$item"
                                    fi
                                done
                                echo "Completed Stage 2 DynamoDB cleanup"
                            else
                                echo "No Stage 2 DynamoDB entries found to clean up"
                            fi
                            
                            # Wait for DynamoDB to process
                            sleep 15
                            
                            # Verify Stage 2 DynamoDB cleanup
                            echo "Verifying Stage 2 DynamoDB cleanup..."
                            REMAINING_ITEMS=\$(aws dynamodb scan --table-name terraform-state-lock-table --region us-west-2 --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")
                            if [ "\$REMAINING_ITEMS" != "" ] && [ "\$REMAINING_ITEMS" != "None" ]; then
                                echo "Checking remaining items for Stage 2 references: \$REMAINING_ITEMS"
                                for item in \$REMAINING_ITEMS; do
                                    if [ "\$item" != "" ] && [ "\$item" != "None" ] && echo "\$item" | grep -q "stage2"; then
                                        echo "Secondary cleanup of Stage 2 DynamoDB item: \$item"
                                        aws dynamodb delete-item \
                                            --table-name terraform-state-lock-table \
                                            --key "{\\"LockID\\":{\\"S\\":\\"\$item\\"}}" \
                                            --region us-west-2 2>/dev/null || echo "Could not delete \$item in secondary cleanup"
                                    fi
                                done
                                sleep 10
                            else
                                echo "Stage 2 DynamoDB cleanup verified - no stage2 references found"
                            fi
                            
                            # Try init again
                            echo "Retrying Stage 2 terraform init after digest cleanup..."
                            if ! terraform init; then
                                echo "Stage 2 terraform init still failing, attempting state backup and recovery..."
                                
                                # Backup current stage 2 state from S3
                                echo "Backing up current Stage 2 state file..."
                                aws s3 cp s3://west-betech-tfstate/infra/stage2/terraformstatefile ./terraform.tfstate.backup-stage2-\$(date +%Y%m%d_%H%M%S) 2>/dev/null || echo "No Stage 2 state file to backup"
                                
                                # Remove corrupted stage 2 state temporarily
                                echo "Temporarily removing Stage 2 state file for fresh init..."
                                aws s3 rm s3://west-betech-tfstate/infra/stage2/terraformstatefile 2>/dev/null || echo "No Stage 2 state file to remove"
                                
                                # Try init with empty state
                                echo "Initializing Stage 2 with clean state..."
                                if terraform init; then
                                    echo "Successfully initialized Stage 2 with clean state"
                                    echo "Stage 2 will start with fresh state"
                                else
                                    echo "Stage 2 recovery attempts failed. Manual intervention required."
                                    echo "=== STAGE 2 MANUAL RECOVERY STEPS ==="
                                    echo "1. Go to AWS Console -> DynamoDB -> Tables -> terraform-state-lock-table"
                                    echo "2. Delete items with LockID containing 'infra/stage2/terraformstatefile'"
                                    echo "3. Go to AWS Console -> S3 -> west-betech-tfstate bucket"
                                    echo "4. Delete the file: infra/stage2/terraformstatefile (if it exists)"
                                    echo "5. Re-run this pipeline"
                                    echo "========================================"
                                    cd ..
                                    exit 1
                                fi
                            fi
                        else
                            echo "Different Stage 2 terraform init error, investigating..."
                            # Check if it's a permissions or other issue for stage 2
                            terraform init -no-color 2>&1 | tee stage2_init_error.log || {
                                echo "Stage 2 terraform init failed. Error details:"
                                cat stage2_init_error.log
                                echo "Checking AWS credentials and permissions for Stage 2..."
                                aws sts get-caller-identity || echo "AWS credentials issue detected"
                                echo "Checking S3 bucket access for Stage 2..."
                                aws s3 ls s3://west-betech-tfstate/infra/stage2/ || echo "S3 stage2 path access issue detected"
                                echo "Checking DynamoDB table access for Stage 2..."
                                aws dynamodb describe-table --table-name terraform-state-lock-table --region us-west-2 || echo "DynamoDB table access issue detected"
                                cd ..
                                exit 1
                            }
                        fi
                    }
                    
                    echo 'Planning stage 2 deployment...'
                    
                    # Proactively check for existing resources before planning
                    echo "Checking for existing AWS resources before planning..."
                    
                    # Check and import IAM role if it exists
                    if aws iam get-role --role-name betech-alb-controller --region us-west-2 >/dev/null 2>&1; then
                        echo "Found existing IAM role betech-alb-controller, importing before plan..."
                        terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role.this[0]' betech-alb-controller || {
                            echo "Import failed or resource already in state, checking state..."
                            terraform state show 'module.aws_load_balancer_controller_irsa_role.aws_iam_role.this[0]' >/dev/null 2>&1 && echo "Role already in state" || echo "Role import failed, will retry during apply"
                        }
                    fi
                    
                    # Check and import IAM policy attachment if it exists
                    if aws iam list-attached-role-policies --role-name betech-alb-controller --region us-west-2 2>/dev/null | grep -q "AWSLoadBalancerControllerIAMPolicy"; then
                        echo "Found existing policy attachment, importing before plan..."
                        POLICY_ARN=$(
                            aws iam list-attached-role-policies \
                                --role-name betech-alb-controller \
                                --region us-west-2 \
                                --query 'AttachedPolicies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy`].PolicyArn' \
                                --output text 2>/dev/null || echo ""
                        )
                        if [ "$POLICY_ARN" != "" ] && [ "$POLICY_ARN" != "None" ]; then
                            terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role_policy_attachment.custom[0]' "betech-alb-controller/$POLICY_ARN" || {
                                echo "Policy attachment import failed or already in state, checking..."
                                terraform state show 'module.aws_load_balancer_controller_irsa_role.aws_iam_role_policy_attachment.custom[0]' >/dev/null 2>&1 && echo "Policy attachment already in state" || echo "Policy attachment import failed, will retry during apply"
                            }
                        fi
                    fi
                    
                    terraform plan -out=stage2-plan || {
                        echo "Stage 2 terraform plan failed, checking for existing resources..."
                        
                        # Check for common resource conflicts and attempt imports
                        echo "Checking for existing AWS resources that might conflict..."
                        
                        # Check IAM role
                        if aws iam get-role --role-name betech-alb-controller --region us-west-2 >/dev/null 2>&1; then
                            echo "Found existing IAM role betech-alb-controller, attempting to import..."
                            terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role.this[0]' betech-alb-controller || echo "IAM role import failed, continuing..."
                        fi
                        
                        # Check IAM policy attachments
                        if aws iam list-attached-role-policies --role-name betech-alb-controller --region us-west-2 | grep -q "AWSLoadBalancerControllerIAMPolicy"; then
                            echo "Found existing policy attachment, attempting to import..."
                            # Get the policy ARN
                            POLICY_ARN=\$(aws iam list-attached-role-policies --role-name betech-alb-controller --region us-west-2 --query 'AttachedPolicies[?PolicyName==\`AWSLoadBalancerControllerIAMPolicy\`].PolicyArn' --output text)
                            if [ "\$POLICY_ARN" != "" ]; then
                                terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role_policy_attachment.custom[0]' "betech-alb-controller/\$POLICY_ARN" || echo "Policy attachment import failed, continuing..."
                            fi
                        fi
                        
                        # Try plan again after imports
                        echo "Retrying terraform plan after resource imports..."
                        terraform plan -out=stage2-plan || {
                            echo "Plan still failing after import attempts, trying force refresh..."
                            terraform refresh || echo "Refresh failed"
                            
                            # Final plan attempt
                            terraform plan -out=stage2-plan || {
                                echo "Stage 2 terraform plan failed even after import and refresh attempts!"
                                echo "Checking specific error details..."
                                terraform plan -no-color 2>&1 | head -20
                                echo ""
                                echo "=== TROUBLESHOOTING SUGGESTIONS ==="
                                echo "1. Check if ALB Controller is already installed via Helm"
                                echo "2. Verify IAM roles and policies in AWS Console"
                                echo "3. Consider running 'terraform destroy' in stage2-kubernetes to clean up"
                                echo "4. Check for conflicting resources in AWS"
                                echo "===================================="
                                cd ..
                                exit 1
                            }
                        }
                        
                        echo "Terraform plan succeeded after import operations"
                    }
                    
                    echo 'Setting up Helm repositories before applying stage 2...'
                    # Set up Helm repositories required by the AWS Load Balancer Controller
                    helm repo add eks https://aws.github.io/eks-charts || echo "EKS repo already exists"
                    helm repo update
                    
                    echo 'Applying stage 2 resources...'
                    terraform apply stage2-plan || {
                        echo "Stage 2 terraform apply failed, checking for resource conflicts..."
                        
                        # Check if it's a role already exists error
                        if terraform apply stage2-plan 2>&1 | grep -q "Role with name betech-alb-controller already exists"; then
                            echo "IAM role conflict detected, attempting to import existing resources..."
                            
                            # Import the existing IAM role
                            echo "Importing existing IAM role: betech-alb-controller"
                            terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role.this[0]' betech-alb-controller || {
                                echo "Import failed, checking if resource is already in state..."
                                terraform state show 'module.aws_load_balancer_controller_irsa_role.aws_iam_role.this[0]' >/dev/null 2>&1 && echo "Role already in state" || echo "Role import failed"
                            }
                            
                            # Import policy attachment if it exists
                            POLICY_ARN=\$(aws iam list-attached-role-policies --role-name betech-alb-controller --region us-west-2 --query 'AttachedPolicies[?PolicyName==\`AWSLoadBalancerControllerIAMPolicy\`].PolicyArn' --output text 2>/dev/null || echo "")
                            if [ "\$POLICY_ARN" != "" ] && [ "\$POLICY_ARN" != "None" ]; then
                                echo "Importing policy attachment: \$POLICY_ARN"
                                terraform import 'module.aws_load_balancer_controller_irsa_role.aws_iam_role_policy_attachment.custom[0]' "betech-alb-controller/\$POLICY_ARN" || {
                                    echo "Policy attachment import failed, checking if already in state..."
                                    terraform state show 'module.aws_load_balancer_controller_irsa_role.aws_iam_role_policy_attachment.custom[0]' >/dev/null 2>&1 && echo "Policy attachment already in state" || echo "Policy attachment import failed"
                                }
                            fi
                            
                            # Try to apply again after imports
                            echo "Retrying terraform apply after importing existing resources..."
                            terraform apply --auto-approve || {
                                echo "Apply still failing, trying to refresh state and apply again..."
                                terraform refresh
                                terraform apply --auto-approve || {
                                    echo "Stage 2 terraform apply failed even after import attempts!"
                                    echo "Checking current state..."
                                    terraform state list | grep -i alb || echo "No ALB resources in state"
                                    echo "Checking actual AWS resources..."
                                    aws iam get-role --role-name betech-alb-controller --region us-west-2 >/dev/null 2>&1 && echo "IAM role exists in AWS" || echo "IAM role not found in AWS"
                                    cd ..
                                    exit 1
                                }
                            }
                        else
                            echo "Different apply error, failing..."
                            cd ..
                            exit 1
                        fi
                    }
                    
                    echo 'Verifying node authentication and fixing if needed...'
                    # Check if nodes are ready, if not, fix aws-auth ConfigMap
                    if ! kubectl wait --for=condition=Ready nodes --all --timeout=60s; then
                        echo 'Nodes not ready, checking and fixing aws-auth ConfigMap...'
                        
                        # Get node group roles
                        NODE_GROUPS=\$(aws eks list-nodegroups --cluster-name \${cluster_name} --region \${main_region} --query 'nodegroups[]' --output text)
                        
                        # Build aws-auth fix
                        echo 'Updating aws-auth ConfigMap with node group roles...'
                        cat > /tmp/aws-auth-fix.yaml << 'AUTHEOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
AUTHEOF
                        
                        for ng in \$NODE_GROUPS; do
                            ROLE=\$(aws eks describe-nodegroup --cluster-name \${cluster_name} --nodegroup-name \$ng --region \${main_region} --query 'nodegroup.nodeRole' --output text)
                            cat >> /tmp/aws-auth-fix.yaml << AUTHEOF
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: \$ROLE
      username: system:node:{{EC2PrivateDNSName}}
AUTHEOF
                        done
                        
                        # Add user role
                        cat >> /tmp/aws-auth-fix.yaml << 'AUTHEOF'
    - groups:
      - system:masters
      rolearn: arn:aws:iam::${account_id}:role/terraform-poweruser
      username: betech-west
AUTHEOF
                        
                        kubectl apply -f /tmp/aws-auth-fix.yaml
                        rm -f /tmp/aws-auth-fix.yaml
                        
                        echo 'Waiting for nodes to become ready after aws-auth fix...'
                        kubectl wait --for=condition=Ready nodes --all --timeout=180s
                    fi
                    
                    echo 'Final verification of all components...'
                    kubectl get nodes
                    kubectl get pods -n kube-system | grep aws-load-balancer-controller
                    cd ..
                    """
                }
                
                // Apply any remaining resources in main configuration
                echo 'Applying any remaining main configuration resources...'
                sh '''
                echo 'Checking for remaining resources to apply...'
                terraform plan -detailed-exitcode || {
                    PLAN_EXIT_CODE=$?
                    if [ $PLAN_EXIT_CODE -eq 2 ]; then
                        echo "Changes detected, applying remaining resources..."
                        terraform apply --auto-approve
                    elif [ $PLAN_EXIT_CODE -eq 0 ]; then
                        echo "No changes needed in main configuration"
                    else
                        echo "Terraform plan failed!"
                        exit 1
                    fi
                }
                '''
            }
        }
        
        // stage('6. Apply Application Manifests') {
        //     when { 
        //         expression { params.Deployment_Type == 'apply' }
        //     }
        //     steps {
        //         echo 'Applying Kubernetes application manifests...'
                
        //         script {
        //             // Ensure kubectl is configured properly
        //             def cluster_name = 'betech-cluster'
        //             try {
        //                 cluster_name = sh(script: "terraform output -raw cluster_name", returnStdout: true).trim()
        //             } catch (Exception e) {
        //                 echo "Using default cluster name: ${cluster_name}"
        //             }
                    
        //             sh """
        //             echo 'Verifying cluster connectivity before applying manifests...'
        //             kubectl config current-context
        //             kubectl get nodes
                    
        //             echo 'Checking ALB Controller status...'
        //             kubectl get pods -n kube-system | grep aws-load-balancer-controller || echo "ALB Controller not found, but continuing..."
                    
        //             echo 'Applying application manifests with auto-discovery...'
        //             # Use the enhanced apply-manifests script that handles auto-discovery
        //             if [ -f "apply-manifests.sh" ]; then
        //                 chmod +x apply-manifests.sh
        //                 ./apply-manifests.sh
        //             else
        //                 echo "Warning: apply-manifests.sh not found, applying manifests directly..."
                        
        //                 # Fallback: Apply manifests directly
        //                 if [ -d "../weatherappPYTHON-BETECH/manifest" ]; then
        //                     echo "Applying manifests from ../weatherappPYTHON-BETECH/manifest/"
                            
        //                     # Create namespaces if they don't exist
        //                     kubectl create namespace directory --dry-run=client -o yaml | kubectl apply -f -
        //                     kubectl create namespace gateway --dry-run=client -o yaml | kubectl apply -f -
        //                     kubectl create namespace analytics --dry-run=client -o yaml | kubectl apply -f -
                            
        //                     # Apply manifests in order
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/deployment.yaml" ]; then
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/deployment.yaml
        //                     fi
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/service.yaml" ]; then
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/service.yaml
        //                     fi
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/ingress.yaml" ]; then
        //                         echo "Applying ingress with ALB auto-discovery (no hardcoded subnets)..."
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/ingress.yaml
        //                     fi
        //                 else
        //                     echo "Warning: No manifest directory found"
        //                 fi
        //             fi
                    
        //             echo 'Verifying application deployment...'
        //             kubectl get pods -n directory || echo "No pods in directory namespace yet"
        //             kubectl get svc -n directory || echo "No services in directory namespace yet"
        //             kubectl get ingress -n directory || echo "No ingresses in directory namespace yet"
                    
        //             echo 'Checking ALB provisioning status...'
        //             sleep 30  # Give ALB time to provision
        //             kubectl describe ingress -n directory || echo "No ingresses to describe"
                    
        //             echo 'Running ALB auto-discovery verification...'
        //             if [ -f "verify-alb-auto-discovery.sh" ]; then
        //                 chmod +x verify-alb-auto-discovery.sh
        //                 ./verify-alb-auto-discovery.sh || echo "Verification completed with warnings"
        //             fi
        //             """
        //         }
        //     }
        // }
        
        stage('6. Terraform Destroy') {
            when { 
                expression { params.Deployment_Type == 'destroy' }
            }
            steps {
                echo 'Destroying infrastructure safely...'
                
                // First destroy the weather app deployment
                echo 'Step 1: Destroying weather-app-deployment...'
                script {
                    try {
                        build job: 'weather-app-deployment', 
                              parameters: [
                                  string(name: 'Deployment_Type', value: 'destroy')
                              ],
                              wait: true
                        echo 'Weather app deployment destroyed successfully'
                    } catch (Exception e) {
                        echo "Warning: Could not destroy weather-app-deployment: ${e.getMessage()}"
                        echo "Continuing with infrastructure destroy..."
                    }
                }
                
                echo 'Step 2: Destroying EKS infrastructure...'
                
                // Clean up any remaining Kubernetes resources from weather app
                echo 'Step 2.1: Cleaning up remaining Kubernetes resources...'
                script {
                    try {
                        def cluster_name = 'betech-cluster'
                        def main_region = 'us-west-2'
                        
                        // Try to get cluster info from terraform outputs
                        try {
                            cluster_name = sh(script: "terraform output -raw cluster_name 2>/dev/null || echo 'betech-cluster'", returnStdout: true).trim()
                            main_region = sh(script: "terraform output -raw main_region 2>/dev/null || echo 'us-west-2'", returnStdout: true).trim()
                        } catch (Exception e) {
                            echo "Using default cluster values: ${cluster_name}, ${main_region}"
                        }
                        
                        sh """
                        echo 'Configuring kubectl for cleanup...'
                        aws eks update-kubeconfig --region ${main_region} --name ${cluster_name} 2>/dev/null || echo "Could not configure kubectl, cluster may already be destroyed"
                        
                        echo 'Cleaning up weather app resources...'
                        # Remove weather app namespaces and resources
                        kubectl delete namespace directory --ignore-not-found=true || echo "Directory namespace not found"
                        kubectl delete namespace gateway --ignore-not-found=true || echo "Gateway namespace not found" 
                        kubectl delete namespace analytics --ignore-not-found=true || echo "Analytics namespace not found"
                        
                        # Clean up any ingresses that might have ALBs
                        kubectl delete ingress --all --all-namespaces --ignore-not-found=true || echo "No ingresses to clean up"
                        
                        # Wait a bit for ALBs to be cleaned up
                        echo 'Waiting for Load Balancers to be cleaned up...'
                        sleep 30
                        
                        echo 'Kubernetes cleanup completed'
                        """
                    } catch (Exception e) {
                        echo "Warning: Kubernetes cleanup failed: ${e.getMessage()}"
                        echo "Continuing with infrastructure destroy..."
                    }
                }
                
                script {
                    // Function to get terraform output with fallback for destroy stage
                    def getTerraformOutputSafe = { outputName, fallback ->
                        try {
                            // Check if terraform state exists first
                            def stateExists = sh(script: 'terraform state list 2>/dev/null | wc -l', returnStdout: true).trim() as Integer
                            if (stateExists > 0) {
                                def output = sh(script: "terraform output -raw ${outputName} 2>/dev/null || echo '${fallback}'", returnStdout: true).trim()
                                return output != '' ? output : fallback
                            } else {
                                echo "No terraform state found, using fallback for ${outputName}: ${fallback}"
                                return fallback
                            }
                        } catch (Exception e) {
                            echo "Could not get terraform output '${outputName}': ${e.getMessage()}, using fallback: ${fallback}"
                            return fallback
                        }
                    }
                    
                    // Get cluster information before destruction with fallbacks
                    def cluster_name = getTerraformOutputSafe('cluster_name', 'betech-cluster')
                    def main_region = getTerraformOutputSafe('main_region', 'us-west-2')
                    def vpc_id = getTerraformOutputSafe('vpc_id', '')
                    
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // Store these values for use in shell commands
                    env.DESTROY_CLUSTER_NAME = cluster_name
                    env.DESTROY_MAIN_REGION = main_region
                    env.DESTROY_VPC_ID = vpc_id
                    env.DESTROY_ACCOUNT_ID = account_id
                    
                    // Destroy stage 2 first with proper configuration
                    sh """
                    echo 'Destroying Stage 2: Kubernetes resources...'
                    
                    if [ -d "stage2-kubernetes" ]; then
                        cd stage2-kubernetes
                        
                        if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ] || [ -f .terraform/terraform.tfstate ]; then
                            echo "Found stage 2 terraform state, proceeding with destroy..."
                            
                            # Try to get VPC ID from cluster if needed for proper destroy
                            VPC_ID="${env.DESTROY_VPC_ID}"
                            if [ "\$VPC_ID" = "" ] || [ "\$VPC_ID" = "vpc-unknown" ]; then
                                echo "Attempting to retrieve VPC ID from cluster..."
                                VPC_ID=\$(aws eks describe-cluster --name ${env.DESTROY_CLUSTER_NAME} --region ${env.DESTROY_MAIN_REGION} --query 'cluster.resourcesVpcConfig.vpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                echo "Retrieved VPC ID from cluster: \$VPC_ID"
                                
                                # If still no VPC ID, try to find it by tags
                                if [ "\$VPC_ID" = "vpc-unknown" ] || [ "\$VPC_ID" = "" ]; then
                                    echo "Trying to find VPC by tags..."
                                    VPC_ID=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                    if [ "\$VPC_ID" = "None" ]; then
                                        VPC_ID="vpc-unknown"
                                    fi
                                    echo "VPC ID from tags: \$VPC_ID"
                                fi
                                
                                # If still no VPC ID, try to get the first available VPC
                                if [ "\$VPC_ID" = "vpc-unknown" ] || [ "\$VPC_ID" = "" ]; then
                                    echo "Trying to get first available VPC..."
                                    VPC_ID=\$(aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                    if [ "\$VPC_ID" = "None" ]; then
                                        VPC_ID="vpc-unknown"
                                    fi
                                    echo "First available VPC ID: \$VPC_ID"
                                fi
                            fi
                            
                            # Create terraform.tfvars for proper destruction
                            cat > terraform.tfvars <<EOF
main-region = "${env.DESTROY_MAIN_REGION}"
env_name = "betech"
cluster_name = "${env.DESTROY_CLUSTER_NAME}"
rolearn = "arn:aws:iam::${env.DESTROY_ACCOUNT_ID}:role/terraform-poweruser"
EOF
                            
                            # Add VPC ID only if valid - with proper syntax
                            if [ "\$VPC_ID" != "" ] && [ "\$VPC_ID" != "vpc-unknown" ] && [ "\$VPC_ID" != "None" ]; then
                                echo "vpc_id = \"\$VPC_ID\"" >> terraform.tfvars
                                echo "Added VPC ID to terraform.tfvars: \$VPC_ID"
                            else
                                echo "Warning: VPC ID not available, trying to retrieve from AWS..."
                                # Try one more time to get VPC ID directly
                                FALLBACK_VPC=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
                                if [ "\$FALLBACK_VPC" != "" ] && [ "\$FALLBACK_VPC" != "None" ]; then
                                    echo "vpc_id = \"\$FALLBACK_VPC\"" >> terraform.tfvars
                                    echo "Using fallback VPC ID: \$FALLBACK_VPC"
                                else
                                    echo "Could not determine VPC ID, skipping vpc_id variable"
                                fi
                            fi
                            
                            echo "Stage 2 terraform.tfvars for destroy:"
                            cat terraform.tfvars
                            
                            echo 'Setting up Helm for destroy operations...'
                            # Initialize Helm if needed for destroy
                            if ! helm version >/dev/null 2>&1; then
                                echo "Helm not found, installing for cleanup..."
                                curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
                            fi
                            
                            # Add required Helm repositories for proper destroy
                            helm repo list | grep eks || helm repo add eks https://aws.github.io/eks-charts
                            helm repo update
                            
                            echo 'Initializing stage 2 terraform for destroy...'
                            terraform init -input=false || {
                                echo "Terraform init failed, cleaning and retrying..."
                                rm -rf .terraform/
                                terraform init -input=false
                            }
                            
                            echo 'Destroying stage 2 resources...'
                            # Check if vpc_id is in tfvars file
                            if grep -q "vpc_id" terraform.tfvars; then
                                echo "VPC ID found in terraform.tfvars, proceeding with normal destroy..."
                                terraform destroy --auto-approve || echo "Stage 2 destroy completed with warnings"
                            else
                                echo "VPC ID not found, attempting destroy with variable override..."
                                # Try to get VPC ID one more time for destroy
                                DESTROY_VPC=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
                                if [ "\$DESTROY_VPC" != "" ] && [ "\$DESTROY_VPC" != "None" ]; then
                                    echo "Using VPC ID for destroy: \$DESTROY_VPC"
                                    terraform destroy -var="vpc_id=\$DESTROY_VPC" --auto-approve || echo "Stage 2 destroy completed with warnings"
                                else
                                    echo "Could not determine VPC ID, attempting destroy without vpc_id..."
                                    # Create a temporary tfvars without vpc_id
                                    cp terraform.tfvars terraform.tfvars.backup
                                    grep -v "vpc_id" terraform.tfvars.backup > terraform.tfvars || true
                                    terraform destroy --auto-approve || echo "Stage 2 destroy completed with warnings"
                                fi
                            fi
                            
                            # Clean up stage 2 state
                            rm -f terraform.tfstate*
                            rm -f terraform.tfvars
                            rm -rf .terraform/
                            rm -f .terraform.lock.hcl
                        else
                            echo "No stage 2 state found, skipping terraform destroy"
                        fi
                        cd ..
                    else
                        echo "Stage 2 directory not found, skipping stage 2 destroy"
                    fi
                    """
                }
                
                // Then destroy stage 1
                sh """
                echo 'Destroying Stage 1: EKS cluster and VPC...'
                if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ] || [ -f .terraform/terraform.tfstate ]; then
                    echo "Found main terraform state, proceeding with destroy..."
                    
                    echo 'Configuring kubectl for destroy operations...'
                    CLUSTER_NAME="${env.DESTROY_CLUSTER_NAME}"
                    aws eks update-kubeconfig --region ${env.DESTROY_MAIN_REGION} --name \$CLUSTER_NAME 2>/dev/null || echo "Could not update kubeconfig, continuing..."
                    
                    echo 'Initializing main terraform for destroy...'
                    terraform init -input=false || {
                        echo "Terraform init failed, cleaning and retrying..."
                        rm -rf .terraform/
                        terraform init -input=false
                    }
                    
                    echo 'Destroying main infrastructure...'
                    terraform destroy --auto-approve || echo "Stage 1 destroy completed with warnings"
                else
                    echo "No main state found, skipping terraform destroy"
                fi
                """
                
                // Clean up all state files and remote state
                sh '''
                echo 'Cleaning up all state files...'
                
                # Local state cleanup
                rm -f terraform.tfstate*
                rm -f .terraform.lock.hcl
                rm -f tfplan
                rm -rf .terraform/
                
                # Stage 2 cleanup
                if [ -d "stage2-kubernetes" ]; then
                    cd stage2-kubernetes
                    rm -f terraform.tfstate*
                    rm -f .terraform.lock.hcl
                    rm -f terraform.tfvars
                    rm -f stage2-plan
                    rm -rf .terraform/
                    cd ..
                fi
                
                echo 'Cleaning remote state from S3...'
                # Try to clean remote state with better error handling
                aws s3api head-object --bucket west-betech-tfstate --key infra/terraformstatefile >/dev/null 2>&1 && {
                    echo "Removing main remote state..."
                    aws s3 rm s3://west-betech-tfstate/infra/terraformstatefile
                } || echo "Main remote state not found or already deleted"
                
                aws s3api head-object --bucket west-betech-tfstate --key infra/stage2/terraformstatefile >/dev/null 2>&1 && {
                    echo "Removing stage 2 remote state..."
                    aws s3 rm s3://west-betech-tfstate/infra/stage2/terraformstatefile
                } || echo "Stage 2 remote state not found or already deleted"
                
                echo 'Cleanup completed successfully'
                '''
            }
        }

        stage('7. Post-deployment Scripts') {
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps {
                echo 'Running post-deployment scripts'
                sh 'scripts/install_helm.sh'
            }
        }
        
        stage('8. Email Notification') {
            steps {
               echo 'Success for BETECH'
               mail bcc: 'betechincorporated@gmail.com', 
                    body: """Terraform ${params.Deployment_Type} deployment is completed.

${params.Deployment_Type == 'destroy' ? 
'Infrastructure destruction completed in the following order:\n1. Weather App Deployment destroyed\n2. Kubernetes resources cleaned up\n3. EKS cluster and VPC destroyed\n4. All state files cleaned up' : 
'Infrastructure deployment completed successfully.\nEKS cluster and ALB controller are ready.\nWeather app deployment will be triggered next.'}
                    
Let me know if the changes look okay.

Thanks,
BETECH Solutions,
+1 (123) 123-4567""", 
                    cc: 'betechincorporated@gmail.com', 
                    from: '', 
                    replyTo: '', 
                    subject: "Terraform ${params.Deployment_Type} deployment completed!!!", 
                    to: 'betechincorporated@gmail.com'
            }
        }
    }       
    post {
        success {
            script {
                if (params.Deployment_Type == 'apply') {
                    // Triggering the weather-app-deployment build only on successful apply
                    build job: 'weather-app-deployment'
                }
            }
        }
        failure {
            echo 'Build failed, not triggering deployment.'
        }
        always {
            echo 'Slack Notifications.'
            slackSend channel: '#all-weatherapp-cicd',
                color: COLOR_MAP[currentBuild.currentResult],
                message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
        }
    }
} 
