def COLOR_MAP = [
    'SUCCESS': 'good', 
    'FAILURE': 'danger',
]

pipeline {
    agent { node { label 'TERRAFORM' } } 
    parameters {
        choice(name: 'Deployment_Type', choices:['apply','destroy'], description:'The deployment type')
        choice(name: 'Manual_Approval', choices: ['Approve','Reject'], description: 'Approve or Reject the deployment')
    }
    environment {
        EMAIL_TO = 'betechincorporated@gmail.com'
        AWS_REGION = 'us-west-2'
    }
    stages {
        stage('1. Terraform Init') {
            steps {
                echo 'Terraform init phase'
                sh 'terraform init'
            }
        }
        
        stage('2. Terraform Plan') {
            steps {
                echo 'Terraform plan phase'
                sh 'terraform plan'
            }
        }

        stage('3. Manual Approval') {
            steps {
                script {
                    def Manual_Approval = params.Manual_Approval
                    echo "Deployment ${Manual_Approval}"

                    if (Manual_Approval == 'Reject') {
                        error "Deployment rejected, stopping pipeline."
                    } 
                }  
            }
        }

        stage('4. Terraform Deploy - Stage 1 (Infrastructure)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 1: VPC and EKS cluster'  
                sh 'terraform apply -target=module.vpc -target=module.eks --auto-approve'
                
                echo 'Configuring kubectl'
                sh 'aws eks update-kubeconfig --region ${AWS_REGION} --name betech-cluster'
                
                echo 'Waiting for cluster to be ready'
                sh 'kubectl wait --for=condition=Ready nodes --all --timeout=600s || echo "Continuing..."'
            }
        }
        
        stage('5. Terraform Deploy - Stage 2 (Kubernetes)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 2: Kubernetes resources and ALB Controller'
                
                script {
                    // Get outputs from stage 1
                    def cluster_name = sh(script: 'terraform output -raw cluster_name', returnStdout: true).trim()
                    def main_region = sh(script: 'terraform output -raw main_region', returnStdout: true).trim()
                    def vpc_id = sh(script: 'terraform output -raw vpc_id', returnStdout: true).trim()
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // Ensure kubectl is properly configured and working
                    sh """
                    echo 'Verifying kubectl configuration...'
                    kubectl config current-context
                    kubectl get nodes
                    kubectl cluster-info
                    """
                    
                    // Create terraform.tfvars for stage 2
                    sh """
                    cd stage2-kubernetes
                    cat > terraform.tfvars <<EOF
main-region = "${main_region}"
env_name = "betech"
cluster_name = "${cluster_name}"
vpc_id = "${vpc_id}"
rolearn = "arn:aws:iam::${account_id}:role/terraform-poweruser"
EOF
                    
                    # Set environment variables for Kubernetes/Helm providers
                    export KUBE_CONFIG_PATH=\$HOME/.kube/config
                    export KUBECONFIG=\$HOME/.kube/config
                    
                    echo 'Initializing stage 2 terraform...'
                    terraform init
                    
                    echo 'Planning stage 2 deployment...'
                    terraform plan
                    
                    echo 'Applying stage 2 resources...'
                    terraform apply --auto-approve
                    
                    echo 'Verifying node authentication and fixing if needed...'
                    # Check if nodes are ready, if not, fix aws-auth ConfigMap
                    if ! kubectl wait --for=condition=Ready nodes --all --timeout=60s; then
                        echo 'Nodes not ready, checking and fixing aws-auth ConfigMap...'
                        
                        # Get node group roles
                        NODE_GROUPS=\$(aws eks list-nodegroups --cluster-name ${cluster_name} --region ${main_region} --query 'nodegroups[]' --output text)
                        
                        # Build aws-auth fix
                        echo 'Updating aws-auth ConfigMap with node group roles...'
                        cat > /tmp/aws-auth-fix.yaml << 'AUTHEOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
AUTHEOF
                        
                        for ng in \$NODE_GROUPS; do
                            ROLE=\$(aws eks describe-nodegroup --cluster-name ${cluster_name} --nodegroup-name \$ng --region ${main_region} --query 'nodegroup.nodeRole' --output text)
                            cat >> /tmp/aws-auth-fix.yaml << AUTHEOF
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: \$ROLE
      username: system:node:{{EC2PrivateDNSName}}
AUTHEOF
                        done
                        
                        # Add user role
                        cat >> /tmp/aws-auth-fix.yaml << 'AUTHEOF'
    - groups:
      - system:masters
      rolearn: arn:aws:iam::${account_id}:role/terraform-poweruser
      username: betech-west
AUTHEOF
                        
                        kubectl apply -f /tmp/aws-auth-fix.yaml
                        rm -f /tmp/aws-auth-fix.yaml
                        
                        echo 'Waiting for nodes to become ready after aws-auth fix...'
                        kubectl wait --for=condition=Ready nodes --all --timeout=180s
                    fi
                    
                    echo 'Final verification of all components...'
                    kubectl get nodes
                    kubectl get pods -n kube-system | grep aws-load-balancer-controller
                    cd ..
                    """
                }
                
                // Apply any remaining resources in main configuration
                sh 'terraform apply --auto-approve'
            }
        }
        
        stage('6. Terraform Destroy') {
            when { 
                expression { params.Deployment_Type == 'destroy' }
            }
            steps {
                echo 'Destroying infrastructure safely...'
                
                script {
                    // Get cluster information before destruction
                    def cluster_name = "betech-cluster"
                    def main_region = "us-west-2"
                    
                    try {
                        cluster_name = sh(script: 'terraform output -raw cluster_name', returnStdout: true).trim()
                        main_region = sh(script: 'terraform output -raw main_region', returnStdout: true).trim()
                    } catch (Exception e) {
                        echo "Could not get terraform outputs, using defaults"
                    }
                    
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // Destroy stage 2 first with proper configuration
                    sh """
                    echo 'Destroying Stage 2: Kubernetes resources...'
                    cd stage2-kubernetes
                    
                    if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ]; then
                        # Create terraform.tfvars for proper destruction
                        VPC_ID=\$(aws eks describe-cluster --name ${cluster_name} --region ${main_region} --query 'cluster.resourcesVpcConfig.vpcId' --output text 2>/dev/null || echo "vpc-unknown")
                        
                        cat > terraform.tfvars <<EOF
main-region = "${main_region}"
env_name = "betech"
cluster_name = "${cluster_name}"
vpc_id = "\$VPC_ID"
rolearn = "arn:aws:iam::${account_id}:role/terraform-poweruser"
EOF
                        
                        terraform init -input=false
                        terraform destroy --auto-approve || echo "Stage 2 destroy completed with warnings"
                        
                        # Clean up stage 2 state
                        rm -f terraform.tfstate*
                        rm -f terraform.tfvars
                        rm -rf .terraform/
                        rm -f .terraform.lock.hcl
                    else
                        echo "No stage 2 state found, skipping"
                    fi
                    cd ..
                    """
                }
                
                // Then destroy stage 1
                sh '''
                echo 'Destroying Stage 1: EKS cluster and VPC...'
                if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ]; then
                    terraform init -input=false
                    terraform destroy --auto-approve || echo "Stage 1 destroy completed with warnings"
                else
                    echo "No main state found, skipping terraform destroy"
                fi
                '''
                
                // Clean up all state files
                sh '''
                echo 'Cleaning up state files...'
                rm -f terraform.tfstate*
                rm -f .terraform.lock.hcl
                rm -rf .terraform/
                rm -f stage2-kubernetes/terraform.tfstate*
                rm -f stage2-kubernetes/.terraform.lock.hcl
                rm -f stage2-kubernetes/terraform.tfvars
                rm -rf stage2-kubernetes/.terraform/
                
                echo 'Cleaning remote state...'
                aws s3 rm s3://west-betech-tfstate/infra/terraformstatefile 2>/dev/null || true
                aws s3 rm s3://west-betech-tfstate/infra/stage2/terraformstatefile 2>/dev/null || true
                '''
            }
        }

        stage('7. Post-deployment Scripts') {
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps {
                echo 'Running post-deployment scripts'
                sh 'scripts/install_helm.sh'
            }
        }
        
        stage('8. Email Notification') {
            steps {
               echo 'Success for BETECH'
               mail bcc: 'betechincorporated@gmail.com', 
                    body: """Terraform ${params.Deployment_Type} deployment is completed.
                    
Let me know if the changes look okay.

Thanks,
BETECH Solutions,
+1 (123) 123-4567""", 
                    cc: 'betechincorporated@gmail.com', 
                    from: '', 
                    replyTo: '', 
                    subject: "Terraform ${params.Deployment_Type} deployment completed!!!", 
                    to: 'betechincorporated@gmail.com'
            }
        }
    }       
    post {
        success {
            script {
                if (params.Deployment_Type == 'apply') {
                    // Triggering the weather-app-deployment build only on successful apply
                    build job: 'weather-app-deployment'
                }
            }
        }
        failure {
            echo 'Build failed, not triggering deployment.'
        }
        always {
            echo 'Slack Notifications.'
            slackSend channel: '#all-weatherapp-cicd',
                color: COLOR_MAP[currentBuild.currentResult],
                message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
        }
    }
} 
