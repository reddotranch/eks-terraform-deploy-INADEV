def COLOR_MAP = [
    'SUCCESS': 'good', 
    'FAILURE': 'danger',
]

pipeline {
    agent { node { label 'TERRAFORM' } } 
    parameters {
        choice(name: 'Deployment_Type', choices:['apply','destroy'], description:'The deployment type')
        choice(name: 'Manual_Approval', choices: ['Approve','Reject'], description: 'Approve or Reject the deployment')
    }
    environment {
        EMAIL_TO = 'betechincorporated@gmail.com'
        AWS_REGION = 'us-west-2'
    }
    stages {
        stage('1. Terraform Init') {
            steps {
                echo 'Terraform init phase'
                sh '''
                echo 'Initializing main terraform configuration...'
                terraform init || {
                    echo "Initial terraform init failed, cleaning and retrying..."
                    rm -rf .terraform/
                    rm -f .terraform.lock.hcl
                    terraform init
                }
                
                echo 'Terraform initialization completed successfully'
                '''
            }
        }
        
        stage('2. Terraform Plan') {
            steps {
                echo 'Terraform plan phase'
                sh '''
                echo 'Running terraform plan for main configuration...'
                terraform plan -out=tfplan || {
                    echo "Terraform plan failed!"
                    exit 1
                }
                
                echo 'Terraform plan completed successfully'
                echo 'Plan summary:'
                terraform show -no-color tfplan | head -20
                '''
            }
        }

        stage('3. Manual Approval') {
            steps {
                script {
                    def Manual_Approval = params.Manual_Approval
                    echo "Deployment ${Manual_Approval}"

                    if (Manual_Approval == 'Reject') {
                        error "Deployment rejected, stopping pipeline."
                    } 
                }  
            }
        }

        stage('4. Terraform Deploy - Stage 1 (Infrastructure)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 1: VPC and EKS cluster'  
                sh '''
                echo 'Applying terraform plan for VPC and EKS cluster...'
                terraform apply -target=module.vpc -target=module.eks tfplan || {
                    echo "Stage 1 deployment failed!"
                    exit 1
                }
                
                echo 'Stage 1 deployment completed successfully'
                echo 'Checking terraform outputs...'
                terraform output || echo "No outputs available yet"
                '''
                
                echo 'Configuring kubectl'
                sh '''
                # Get cluster name with fallback
                CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "betech-cluster")
                echo "Configuring kubectl for cluster: $CLUSTER_NAME"
                
                aws eks update-kubeconfig --region ${AWS_REGION} --name $CLUSTER_NAME || {
                    echo "Failed to update kubeconfig for $CLUSTER_NAME"
                    exit 1
                }
                
                echo "Kubectl configured successfully"
                kubectl config current-context
                '''
                
                echo 'Waiting for cluster to be ready'
                sh '''
                echo "Waiting for cluster nodes to be ready..."
                kubectl wait --for=condition=Ready nodes --all --timeout=600s || {
                    echo "Nodes not ready yet, but continuing with deployment..."
                    kubectl get nodes || echo "Cannot get nodes yet"
                }
                '''
            }
        }
        
        stage('5. Terraform Deploy - Stage 2 (Kubernetes)') {              
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps { 
                echo 'Deploying Stage 2: Kubernetes resources and ALB Controller'
                
                script {
                    // Function to get terraform output with fallback
                    def getTerraformOutput = { outputName, fallback ->
                        try {
                            def output = sh(script: "terraform output -raw ${outputName}", returnStdout: true).trim()
                            if (output && output != 'null' && output != '') {
                                return output
                            } else {
                                echo "Warning: Output '${outputName}' is empty, using fallback: ${fallback}"
                                return fallback
                            }
                        } catch (Exception e) {
                            echo "Warning: Could not get terraform output '${outputName}': ${e.getMessage()}"
                            echo "Using fallback: ${fallback}"
                            return fallback
                        }
                    }
                    
                    // Get outputs from stage 1 with fallbacks
                    def cluster_name = getTerraformOutput('cluster_name', 'betech-cluster')
                    def main_region = getTerraformOutput('main_region', 'us-west-2')
                    def vpc_id = getTerraformOutput('vpc_id', '')
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // If VPC ID is still empty, try to get it from the cluster
                    if (!vpc_id || vpc_id == '') {
                        echo "VPC ID not found in terraform outputs, retrieving from cluster..."
                        try {
                            vpc_id = sh(script: "aws eks describe-cluster --name ${cluster_name} --region ${main_region} --query 'cluster.resourcesVpcConfig.vpcId' --output text", returnStdout: true).trim()
                            echo "Retrieved VPC ID from cluster: ${vpc_id}"
                        } catch (Exception e) {
                            echo "Warning: Could not retrieve VPC ID from cluster: ${e.getMessage()}"
                            vpc_id = "vpc-unknown"
                        }
                    }
                    
                    echo "Using values: cluster=${cluster_name}, region=${main_region}, vpc=${vpc_id}"
                    
                    // Ensure kubectl is properly configured and working
                    sh """
                    echo 'Verifying kubectl configuration...'
                    kubectl config current-context
                    kubectl get nodes
                    kubectl cluster-info
                    """
                    
                    // Create terraform.tfvars for stage 2
                    sh """
                    # Ensure stage2-kubernetes directory exists
                    if [ ! -d "stage2-kubernetes" ]; then
                        echo "Error: stage2-kubernetes directory not found!"
                        exit 1
                    fi
                    
                    cd stage2-kubernetes
                    
                    # Create terraform.tfvars with proper validation
                    echo "Creating terraform.tfvars for stage 2..."
                    cat > terraform.tfvars <<EOF
main-region = "${main_region}"
env_name = "betech"
cluster_name = "${cluster_name}"
EOF
                    
                    # Add VPC ID only if it's valid
                    if [ "${vpc_id}" != "" ] && [ "${vpc_id}" != "vpc-unknown" ]; then
                        echo 'vpc_id = "${vpc_id}"' >> terraform.tfvars
                        echo "Added VPC ID to terraform.tfvars: ${vpc_id}"
                    else
                        echo "Warning: VPC ID not available or invalid, stage2 may need to discover it"
                    fi
                    
                    echo 'rolearn = "arn:aws:iam::${account_id}:role/terraform-poweruser"' >> terraform.tfvars
                    
                    echo "Contents of terraform.tfvars:"
                    cat terraform.tfvars
                    
                    # Set environment variables for Kubernetes/Helm providers
                    export KUBE_CONFIG_PATH=\$HOME/.kube/config
                    export KUBECONFIG=\$HOME/.kube/config
                    
                    echo 'Setting up Helm for AWS Load Balancer Controller...'
                    # Initialize Helm if needed
                    if ! helm version >/dev/null 2>&1; then
                        echo "Helm not found, installing..."
                        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
                    fi
                    
                    # Add required Helm repositories
                    helm repo list | grep eks || helm repo add eks https://aws.github.io/eks-charts
                    helm repo update
                    
                    echo 'Initializing stage 2 terraform...'
                    terraform init || {
                        echo "Stage 2 terraform init failed, cleaning and retrying..."
                        rm -rf .terraform/
                        terraform init
                    }
                    
                    echo 'Planning stage 2 deployment...'
                    terraform plan -out=stage2-plan || {
                        echo "Stage 2 terraform plan failed!"
                        cd ..
                        exit 1
                    }
                    
                    echo 'Setting up Helm repositories before applying stage 2...'
                    # Set up Helm repositories required by the AWS Load Balancer Controller
                    helm repo add eks https://aws.github.io/eks-charts || echo "EKS repo already exists"
                    helm repo update
                    
                    echo 'Applying stage 2 resources...'
                    terraform apply stage2-plan || {
                        echo "Stage 2 terraform apply failed!"
                        cd ..
                        exit 1
                    }
                    
                    echo 'Verifying node authentication and fixing if needed...'
                    # Check if nodes are ready, if not, fix aws-auth ConfigMap
                    if ! kubectl wait --for=condition=Ready nodes --all --timeout=60s; then
                        echo 'Nodes not ready, checking and fixing aws-auth ConfigMap...'
                        
                        # Get node group roles
                        NODE_GROUPS=\$(aws eks list-nodegroups --cluster-name ${cluster_name} --region ${main_region} --query 'nodegroups[]' --output text)
                        
                        # Build aws-auth fix
                        echo 'Updating aws-auth ConfigMap with node group roles...'
                        cat > /tmp/aws-auth-fix.yaml << 'AUTHEOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
AUTHEOF
                        
                        for ng in \$NODE_GROUPS; do
                            ROLE=\$(aws eks describe-nodegroup --cluster-name ${cluster_name} --nodegroup-name \$ng --region ${main_region} --query 'nodegroup.nodeRole' --output text)
                            cat >> /tmp/aws-auth-fix.yaml << AUTHEOF
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: \$ROLE
      username: system:node:{{EC2PrivateDNSName}}
AUTHEOF
                        done
                        
                        # Add user role
                        cat >> /tmp/aws-auth-fix.yaml << 'AUTHEOF'
    - groups:
      - system:masters
      rolearn: arn:aws:iam::${account_id}:role/terraform-poweruser
      username: betech-west
AUTHEOF
                        
                        kubectl apply -f /tmp/aws-auth-fix.yaml
                        rm -f /tmp/aws-auth-fix.yaml
                        
                        echo 'Waiting for nodes to become ready after aws-auth fix...'
                        kubectl wait --for=condition=Ready nodes --all --timeout=180s
                    fi
                    
                    echo 'Final verification of all components...'
                    kubectl get nodes
                    kubectl get pods -n kube-system | grep aws-load-balancer-controller
                    cd ..
                    """
                }
                
                // Apply any remaining resources in main configuration
                echo 'Applying any remaining main configuration resources...'
                sh '''
                echo 'Checking for remaining resources to apply...'
                terraform plan -detailed-exitcode || {
                    PLAN_EXIT_CODE=$?
                    if [ $PLAN_EXIT_CODE -eq 2 ]; then
                        echo "Changes detected, applying remaining resources..."
                        terraform apply --auto-approve
                    elif [ $PLAN_EXIT_CODE -eq 0 ]; then
                        echo "No changes needed in main configuration"
                    else
                        echo "Terraform plan failed!"
                        exit 1
                    fi
                }
                '''
            }
        }
        
        // stage('6. Apply Application Manifests') {
        //     when { 
        //         expression { params.Deployment_Type == 'apply' }
        //     }
        //     steps {
        //         echo 'Applying Kubernetes application manifests...'
                
        //         script {
        //             // Ensure kubectl is configured properly
        //             def cluster_name = 'betech-cluster'
        //             try {
        //                 cluster_name = sh(script: "terraform output -raw cluster_name", returnStdout: true).trim()
        //             } catch (Exception e) {
        //                 echo "Using default cluster name: ${cluster_name}"
        //             }
                    
        //             sh """
        //             echo 'Verifying cluster connectivity before applying manifests...'
        //             kubectl config current-context
        //             kubectl get nodes
                    
        //             echo 'Checking ALB Controller status...'
        //             kubectl get pods -n kube-system | grep aws-load-balancer-controller || echo "ALB Controller not found, but continuing..."
                    
        //             echo 'Applying application manifests with auto-discovery...'
        //             # Use the enhanced apply-manifests script that handles auto-discovery
        //             if [ -f "apply-manifests.sh" ]; then
        //                 chmod +x apply-manifests.sh
        //                 ./apply-manifests.sh
        //             else
        //                 echo "Warning: apply-manifests.sh not found, applying manifests directly..."
                        
        //                 # Fallback: Apply manifests directly
        //                 if [ -d "../weatherappPYTHON-BETECH/manifest" ]; then
        //                     echo "Applying manifests from ../weatherappPYTHON-BETECH/manifest/"
                            
        //                     # Create namespaces if they don't exist
        //                     kubectl create namespace directory --dry-run=client -o yaml | kubectl apply -f -
        //                     kubectl create namespace gateway --dry-run=client -o yaml | kubectl apply -f -
        //                     kubectl create namespace analytics --dry-run=client -o yaml | kubectl apply -f -
                            
        //                     # Apply manifests in order
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/deployment.yaml" ]; then
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/deployment.yaml
        //                     fi
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/service.yaml" ]; then
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/service.yaml
        //                     fi
        //                     if [ -f "../weatherappPYTHON-BETECH/manifest/ingress.yaml" ]; then
        //                         echo "Applying ingress with ALB auto-discovery (no hardcoded subnets)..."
        //                         kubectl apply -f ../weatherappPYTHON-BETECH/manifest/ingress.yaml
        //                     fi
        //                 else
        //                     echo "Warning: No manifest directory found"
        //                 fi
        //             fi
                    
        //             echo 'Verifying application deployment...'
        //             kubectl get pods -n directory || echo "No pods in directory namespace yet"
        //             kubectl get svc -n directory || echo "No services in directory namespace yet"
        //             kubectl get ingress -n directory || echo "No ingresses in directory namespace yet"
                    
        //             echo 'Checking ALB provisioning status...'
        //             sleep 30  # Give ALB time to provision
        //             kubectl describe ingress -n directory || echo "No ingresses to describe"
                    
        //             echo 'Running ALB auto-discovery verification...'
        //             if [ -f "verify-alb-auto-discovery.sh" ]; then
        //                 chmod +x verify-alb-auto-discovery.sh
        //                 ./verify-alb-auto-discovery.sh || echo "Verification completed with warnings"
        //             fi
        //             """
        //         }
        //     }
        // }
        
        stage('6. Terraform Destroy') {
            when { 
                expression { params.Deployment_Type == 'destroy' }
            }
            steps {
                echo 'Destroying infrastructure safely...'
                
                // First destroy the weather app deployment
                echo 'Step 1: Destroying weather-app-deployment...'
                script {
                    try {
                        build job: 'weather-app-deployment', 
                              parameters: [
                                  string(name: 'Deployment_Type', value: 'destroy')
                              ],
                              wait: true
                        echo 'Weather app deployment destroyed successfully'
                    } catch (Exception e) {
                        echo "Warning: Could not destroy weather-app-deployment: ${e.getMessage()}"
                        echo "Continuing with infrastructure destroy..."
                    }
                }
                
                echo 'Step 2: Destroying EKS infrastructure...'
                
                // Clean up any remaining Kubernetes resources from weather app
                echo 'Step 2.1: Cleaning up remaining Kubernetes resources...'
                script {
                    try {
                        def cluster_name = 'betech-cluster'
                        def main_region = 'us-west-2'
                        
                        // Try to get cluster info from terraform outputs
                        try {
                            cluster_name = sh(script: "terraform output -raw cluster_name 2>/dev/null || echo 'betech-cluster'", returnStdout: true).trim()
                            main_region = sh(script: "terraform output -raw main_region 2>/dev/null || echo 'us-west-2'", returnStdout: true).trim()
                        } catch (Exception e) {
                            echo "Using default cluster values: ${cluster_name}, ${main_region}"
                        }
                        
                        sh """
                        echo 'Configuring kubectl for cleanup...'
                        aws eks update-kubeconfig --region ${main_region} --name ${cluster_name} 2>/dev/null || echo "Could not configure kubectl, cluster may already be destroyed"
                        
                        echo 'Cleaning up weather app resources...'
                        # Remove weather app namespaces and resources
                        kubectl delete namespace directory --ignore-not-found=true || echo "Directory namespace not found"
                        kubectl delete namespace gateway --ignore-not-found=true || echo "Gateway namespace not found" 
                        kubectl delete namespace analytics --ignore-not-found=true || echo "Analytics namespace not found"
                        
                        # Clean up any ingresses that might have ALBs
                        kubectl delete ingress --all --all-namespaces --ignore-not-found=true || echo "No ingresses to clean up"
                        
                        # Wait a bit for ALBs to be cleaned up
                        echo 'Waiting for Load Balancers to be cleaned up...'
                        sleep 30
                        
                        echo 'Kubernetes cleanup completed'
                        """
                    } catch (Exception e) {
                        echo "Warning: Kubernetes cleanup failed: ${e.getMessage()}"
                        echo "Continuing with infrastructure destroy..."
                    }
                }
                
                script {
                    // Function to get terraform output with fallback for destroy stage
                    def getTerraformOutputSafe = { outputName, fallback ->
                        try {
                            // Check if terraform state exists first
                            def stateExists = sh(script: 'terraform state list 2>/dev/null | wc -l', returnStdout: true).trim() as Integer
                            if (stateExists > 0) {
                                def output = sh(script: "terraform output -raw ${outputName} 2>/dev/null || echo '${fallback}'", returnStdout: true).trim()
                                return output != '' ? output : fallback
                            } else {
                                echo "No terraform state found, using fallback for ${outputName}: ${fallback}"
                                return fallback
                            }
                        } catch (Exception e) {
                            echo "Could not get terraform output '${outputName}': ${e.getMessage()}, using fallback: ${fallback}"
                            return fallback
                        }
                    }
                    
                    // Get cluster information before destruction with fallbacks
                    def cluster_name = getTerraformOutputSafe('cluster_name', 'betech-cluster')
                    def main_region = getTerraformOutputSafe('main_region', 'us-west-2')
                    def vpc_id = getTerraformOutputSafe('vpc_id', '')
                    
                    def account_id = sh(script: 'aws sts get-caller-identity --query Account --output text', returnStdout: true).trim()
                    
                    // Destroy stage 2 first with proper configuration
                    sh """
                    echo 'Destroying Stage 2: Kubernetes resources...'
                    
                    if [ -d "stage2-kubernetes" ]; then
                        cd stage2-kubernetes
                        
                        if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ] || [ -f .terraform/terraform.tfstate ]; then
                            echo "Found stage 2 terraform state, proceeding with destroy..."
                            
                            # Try to get VPC ID from cluster if needed for proper destroy
                            VPC_ID="${vpc_id}"
                            if [ "\$VPC_ID" = "" ] || [ "\$VPC_ID" = "vpc-unknown" ]; then
                                echo "Attempting to retrieve VPC ID from cluster..."
                                VPC_ID=\$(aws eks describe-cluster --name ${cluster_name} --region ${main_region} --query 'cluster.resourcesVpcConfig.vpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                echo "Retrieved VPC ID from cluster: \$VPC_ID"
                                
                                # If still no VPC ID, try to find it by tags
                                if [ "\$VPC_ID" = "vpc-unknown" ] || [ "\$VPC_ID" = "" ]; then
                                    echo "Trying to find VPC by tags..."
                                    VPC_ID=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                    if [ "\$VPC_ID" = "None" ]; then
                                        VPC_ID="vpc-unknown"
                                    fi
                                    echo "VPC ID from tags: \$VPC_ID"
                                fi
                                
                                # If still no VPC ID, try to get the first available VPC
                                if [ "\$VPC_ID" = "vpc-unknown" ] || [ "\$VPC_ID" = "" ]; then
                                    echo "Trying to get first available VPC..."
                                    VPC_ID=\$(aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "vpc-unknown")
                                    if [ "\$VPC_ID" = "None" ]; then
                                        VPC_ID="vpc-unknown"
                                    fi
                                    echo "First available VPC ID: \$VPC_ID"
                                fi
                            fi
                            
                            # Create terraform.tfvars for proper destruction
                            cat > terraform.tfvars <<EOF
main-region = "${main_region}"
env_name = "betech"
cluster_name = "${cluster_name}"
rolearn = "arn:aws:iam::${account_id}:role/terraform-poweruser"
EOF
                            
                            # Add VPC ID only if valid - with proper syntax
                            if [ "\$VPC_ID" != "" ] && [ "\$VPC_ID" != "vpc-unknown" ] && [ "\$VPC_ID" != "None" ]; then
                                echo "vpc_id = \"\$VPC_ID\"" >> terraform.tfvars
                                echo "Added VPC ID to terraform.tfvars: \$VPC_ID"
                            else
                                echo "Warning: VPC ID not available, trying to retrieve from AWS..."
                                # Try one more time to get VPC ID directly
                                FALLBACK_VPC=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
                                if [ "\$FALLBACK_VPC" != "" ] && [ "\$FALLBACK_VPC" != "None" ]; then
                                    echo "vpc_id = \"\$FALLBACK_VPC\"" >> terraform.tfvars
                                    echo "Using fallback VPC ID: \$FALLBACK_VPC"
                                else
                                    echo "Could not determine VPC ID, skipping vpc_id variable"
                                fi
                            fi
                            
                            echo "Stage 2 terraform.tfvars for destroy:"
                            cat terraform.tfvars
                            
                            echo 'Setting up Helm for destroy operations...'
                            # Initialize Helm if needed for destroy
                            if ! helm version >/dev/null 2>&1; then
                                echo "Helm not found, installing for cleanup..."
                                curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
                            fi
                            
                            # Add required Helm repositories for proper destroy
                            helm repo list | grep eks || helm repo add eks https://aws.github.io/eks-charts
                            helm repo update
                            
                            echo 'Initializing stage 2 terraform for destroy...'
                            terraform init -input=false || {
                                echo "Terraform init failed, cleaning and retrying..."
                                rm -rf .terraform/
                                terraform init -input=false
                            }
                            
                            echo 'Destroying stage 2 resources...'
                            # Check if vpc_id is in tfvars file
                            if grep -q "vpc_id" terraform.tfvars; then
                                echo "VPC ID found in terraform.tfvars, proceeding with normal destroy..."
                                terraform destroy --auto-approve || echo "Stage 2 destroy completed with warnings"
                            else
                                echo "VPC ID not found, attempting destroy with variable override..."
                                # Try to get VPC ID one more time for destroy
                                DESTROY_VPC=\$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*betech*" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
                                if [ "\$DESTROY_VPC" != "" ] && [ "\$DESTROY_VPC" != "None" ]; then
                                    echo "Using VPC ID for destroy: \$DESTROY_VPC"
                                    terraform destroy -var="vpc_id=\$DESTROY_VPC" --auto-approve || echo "Stage 2 destroy completed with warnings"
                                else
                                    echo "Could not determine VPC ID, attempting destroy without vpc_id..."
                                    # Create a temporary tfvars without vpc_id
                                    cp terraform.tfvars terraform.tfvars.backup
                                    grep -v "vpc_id" terraform.tfvars.backup > terraform.tfvars || true
                                    terraform destroy --auto-approve || echo "Stage 2 destroy completed with warnings"
                                fi
                            fi
                            
                            # Clean up stage 2 state
                            rm -f terraform.tfstate*
                            rm -f terraform.tfvars
                            rm -rf .terraform/
                            rm -f .terraform.lock.hcl
                        else
                            echo "No stage 2 state found, skipping terraform destroy"
                        fi
                        cd ..
                    else
                        echo "Stage 2 directory not found, skipping stage 2 destroy"
                    fi
                    """
                }
                
                // Then destroy stage 1
                sh """
                echo 'Destroying Stage 1: EKS cluster and VPC...'
                if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ] || [ -f .terraform/terraform.tfstate ]; then
                    echo "Found main terraform state, proceeding with destroy..."
                    
                    echo 'Configuring kubectl for destroy operations...'
                    CLUSTER_NAME="${cluster_name}"
                    aws eks update-kubeconfig --region ${main_region} --name \$CLUSTER_NAME 2>/dev/null || echo "Could not update kubeconfig, continuing..."
                    
                    echo 'Initializing main terraform for destroy...'
                    terraform init -input=false || {
                        echo "Terraform init failed, cleaning and retrying..."
                        rm -rf .terraform/
                        terraform init -input=false
                    }
                    
                    echo 'Destroying main infrastructure...'
                    terraform destroy --auto-approve || echo "Stage 1 destroy completed with warnings"
                else
                    echo "No main state found, skipping terraform destroy"
                fi
                """
                
                // Clean up all state files and remote state
                sh '''
                echo 'Cleaning up all state files...'
                
                # Local state cleanup
                rm -f terraform.tfstate*
                rm -f .terraform.lock.hcl
                rm -f tfplan
                rm -rf .terraform/
                
                # Stage 2 cleanup
                if [ -d "stage2-kubernetes" ]; then
                    cd stage2-kubernetes
                    rm -f terraform.tfstate*
                    rm -f .terraform.lock.hcl
                    rm -f terraform.tfvars
                    rm -f stage2-plan
                    rm -rf .terraform/
                    cd ..
                fi
                
                echo 'Cleaning remote state from S3...'
                # Try to clean remote state with better error handling
                aws s3api head-object --bucket west-betech-tfstate --key infra/terraformstatefile >/dev/null 2>&1 && {
                    echo "Removing main remote state..."
                    aws s3 rm s3://west-betech-tfstate/infra/terraformstatefile
                } || echo "Main remote state not found or already deleted"
                
                aws s3api head-object --bucket west-betech-tfstate --key infra/stage2/terraformstatefile >/dev/null 2>&1 && {
                    echo "Removing stage 2 remote state..."
                    aws s3 rm s3://west-betech-tfstate/infra/stage2/terraformstatefile
                } || echo "Stage 2 remote state not found or already deleted"
                
                echo 'Cleanup completed successfully'
                '''
            }
        }

        stage('7. Post-deployment Scripts') {
            when { 
                expression { params.Deployment_Type == 'apply' }
            }
            steps {
                echo 'Running post-deployment scripts'
                sh 'scripts/install_helm.sh'
            }
        }
        
        stage('8. Email Notification') {
            steps {
               echo 'Success for BETECH'
               mail bcc: 'betechincorporated@gmail.com', 
                    body: """Terraform ${params.Deployment_Type} deployment is completed.

${params.Deployment_Type == 'destroy' ? 
'Infrastructure destruction completed in the following order:\n1. Weather App Deployment destroyed\n2. Kubernetes resources cleaned up\n3. EKS cluster and VPC destroyed\n4. All state files cleaned up' : 
'Infrastructure deployment completed successfully.\nEKS cluster and ALB controller are ready.\nWeather app deployment will be triggered next.'}
                    
Let me know if the changes look okay.

Thanks,
BETECH Solutions,
+1 (123) 123-4567""", 
                    cc: 'betechincorporated@gmail.com', 
                    from: '', 
                    replyTo: '', 
                    subject: "Terraform ${params.Deployment_Type} deployment completed!!!", 
                    to: 'betechincorporated@gmail.com'
            }
        }
    }       
    post {
        success {
            script {
                if (params.Deployment_Type == 'apply') {
                    // Triggering the weather-app-deployment build only on successful apply
                    build job: 'weather-app-deployment'
                }
            }
        }
        failure {
            echo 'Build failed, not triggering deployment.'
        }
        always {
            echo 'Slack Notifications.'
            slackSend channel: '#all-weatherapp-cicd',
                color: COLOR_MAP[currentBuild.currentResult],
                message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
        }
    }
} 
